{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Basic\n",
    "\n",
    "- Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks(https://arxiv.org/pdf/1511.06434.pdf)\n",
    "\n",
    "<img src=\"./GAN.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla GAN with Multi GPUs + Naming Layers using OrderedDict\n",
    "# Code by GunhoChoi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "# change num_gpu to the number of gpus you want to use\n",
    "\n",
    "epoch = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "num_gpus = 1\n",
    "z_size = 50\n",
    "middle_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "\n",
    "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "\n",
    "# Set Data Loader(input pipeline)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator receives random noise z and create 1x28x28 image\n",
    "# we can name each layer using OrderedDict\n",
    "\n",
    "#2 linear layer\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(z_size,middle_size)),\n",
    "                        ('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.ReLU()),\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,784)),\n",
    "                        #('bn2', nn.BatchNorm2d(784)),\n",
    "                        ('tanh', nn.Tanh()),\n",
    "        ]))\n",
    "    def forward(self,z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size//num_gpus,1,28,28)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator receives 1x28x28 image and returns a float number 0~1\n",
    "# we can name each layer using OrderedDict\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "                        ('fc1',nn.Linear(784,middle_size)),\n",
    "                        #('bn1',nn.BatchNorm1d(middle_size)),\n",
    "                        ('act1',nn.LeakyReLU()),  \n",
    "            \n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "                        ('fc2', nn.Linear(middle_size,1)),\n",
    "                        ('bn2', nn.BatchNorm1d(1)),\n",
    "                        ('act2', nn.Sigmoid()),\n",
    "        ]))\n",
    "                                    \n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size//num_gpus, -1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Put instances on Multi-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put class objects on Multiple GPUs using \n",
    "# torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "# device_ids: default all devices / output_device: default device 0 \n",
    "# along with .cuda()\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.fc1.weight\n",
      "layer1.fc1.bias\n",
      "layer1.bn1.weight\n",
      "layer1.bn1.bias\n",
      "layer1.bn1.running_mean\n",
      "layer1.bn1.running_var\n",
      "layer1.bn1.num_batches_tracked\n",
      "layer2.fc2.weight\n",
      "layer2.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "# Get parameter list by using class.state_dict().keys()\n",
    "\n",
    "gen_params = generator.state_dict().keys()\n",
    "dis_params = discriminator.state_dict().keys()\n",
    "\n",
    "for i in gen_params:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function, optimizers, and labels for training\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "#loss_func = nn.BCELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate,betas=(0.5,0.999))\n",
    "\n",
    "ones_label = Variable(torch.ones(batch_size,1))\n",
    "zeros_label = Variable(torch.zeros(batch_size,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------model restored--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model restore if any\n",
    "\n",
    "try:\n",
    "    generator, discriminator = torch.load('./model/vanilla_gan.pkl')\n",
    "    print(\"\\n--------model restored--------\\n\")\n",
    "except:\n",
    "    print(\"\\n--------model not restored--------\\n\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-9bfd6884ffc4>:11: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  z = Variable(init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1))\n",
      "C:\\Users\\melon\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "<ipython-input-10-9bfd6884ffc4>:24: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  z = Variable(init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5888, grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.2937512993812561 dis_loss: 0.5888055562973022\n",
      "tensor(0.2932, grad_fn=<SumBackward0>) tensor(0.5892, grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.2932169437408447 dis_loss: 0.5891684293746948\n",
      "tensor(0.2944, grad_fn=<SumBackward0>) tensor(0.5911, grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.29437944293022156 dis_loss: 0.5910991430282593\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5907, grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.2942243814468384 dis_loss: 0.590724766254425\n",
      "tensor(0.2906, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.2905969023704529 dis_loss: 0.5911719799041748\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.2937432527542114 dis_loss: 0.5912106037139893\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5848, grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.2927265465259552 dis_loss: 0.5847655534744263\n",
      "tensor(0.2943, grad_fn=<SumBackward0>) tensor(0.5877, grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.2943016588687897 dis_loss: 0.587692379951477\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.29263803362846375 dis_loss: 0.5900892615318298\n",
      "tensor(0.2918, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.29184284806251526 dis_loss: 0.5900658369064331\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.2924603521823883 dis_loss: 0.5890845060348511\n",
      "tensor(0.2950, grad_fn=<SumBackward0>) tensor(0.5923, grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.2949581742286682 dis_loss: 0.5923284292221069\n",
      "tensor(0.2940, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.29397663474082947 dis_loss: 0.590804934501648\n",
      "tensor(0.2944, grad_fn=<SumBackward0>) tensor(0.5932, grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.2944079339504242 dis_loss: 0.5931726694107056\n",
      "tensor(0.2913, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.291282594203949 dis_loss: 0.5912014842033386\n",
      "tensor(0.2902, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.2902136445045471 dis_loss: 0.5891013741493225\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.2938423454761505 dis_loss: 0.5912129878997803\n",
      "tensor(0.2944, grad_fn=<SumBackward0>) tensor(0.5916, grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.29441362619400024 dis_loss: 0.5916008949279785\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5914, grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.29246100783348083 dis_loss: 0.5914297103881836\n",
      "tensor(0.2918, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.2918189465999603 dis_loss: 0.5910258293151855\n",
      "tensor(0.2914, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.2913956046104431 dis_loss: 0.5889608860015869\n",
      "tensor(0.2899, grad_fn=<SumBackward0>) tensor(0.5886, grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.28989407420158386 dis_loss: 0.5885798335075378\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5888, grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.29227304458618164 dis_loss: 0.5888285636901855\n",
      "tensor(0.2941, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.2941281199455261 dis_loss: 0.5904256105422974\n",
      "tensor(0.2922, grad_fn=<SumBackward0>) tensor(0.5905, grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.29216083884239197 dis_loss: 0.5904921293258667\n",
      "tensor(0.2951, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.2950933873653412 dis_loss: 0.5890493392944336\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5889, grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.2925931513309479 dis_loss: 0.588854193687439\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5902, grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.29198259115219116 dis_loss: 0.5901691913604736\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.29392486810684204 dis_loss: 0.5906018018722534\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5880, grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.293334037065506 dis_loss: 0.5879915952682495\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5911, grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.29279404878616333 dis_loss: 0.5910520553588867\n",
      "tensor(0.2913, grad_fn=<SumBackward0>) tensor(0.5872, grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.2912670969963074 dis_loss: 0.5871708393096924\n",
      "tensor(0.2912, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.2912292778491974 dis_loss: 0.5908252000808716\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.2927711606025696 dis_loss: 0.5890305042266846\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.29226747155189514 dis_loss: 0.5900319814682007\n",
      "tensor(0.2910, grad_fn=<SumBackward0>) tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.29104965925216675 dis_loss: 0.5899074077606201\n",
      "tensor(0.2932, grad_fn=<SumBackward0>) tensor(0.5894, grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.29324260354042053 dis_loss: 0.5893811583518982\n",
      "tensor(0.2932, grad_fn=<SumBackward0>) tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.2931725084781647 dis_loss: 0.5899391174316406\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5903, grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.2938336133956909 dis_loss: 0.5902639627456665\n",
      "tensor(0.2893, grad_fn=<SumBackward0>) tensor(0.5885, grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.2893447279930115 dis_loss: 0.5885434150695801\n",
      "tensor(0.2901, grad_fn=<SumBackward0>) tensor(0.5924, grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.29011282324790955 dis_loss: 0.5924274325370789\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5920, grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.29419440031051636 dis_loss: 0.5920450091362\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5889, grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.2938575744628906 dis_loss: 0.5888709425926208\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5928, grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.2937203347682953 dis_loss: 0.5927879214286804\n",
      "tensor(0.2944, grad_fn=<SumBackward0>) tensor(0.5921, grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.29436418414115906 dis_loss: 0.5921010971069336\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5887, grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.2937906086444855 dis_loss: 0.5886602401733398\n",
      "tensor(0.2921, grad_fn=<SumBackward0>) tensor(0.5905, grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.2921385169029236 dis_loss: 0.5904907584190369\n",
      "tensor(0.2949, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.29493990540504456 dis_loss: 0.5882744789123535\n",
      "tensor(0.2930, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.2929595112800598 dis_loss: 0.5898492336273193\n",
      "tensor(0.2943, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.2942587733268738 dis_loss: 0.5907758474349976\n",
      "tensor(0.2940, grad_fn=<SumBackward0>) tensor(0.5923, grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.29402855038642883 dis_loss: 0.5923184156417847\n",
      "tensor(0.2951, grad_fn=<SumBackward0>) tensor(0.5902, grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.2950740456581116 dis_loss: 0.5902478694915771\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.29381564259529114 dis_loss: 0.5897642374038696\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5918, grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.2928493022918701 dis_loss: 0.5917550325393677\n",
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.29350611567497253 dis_loss: 0.5906068086624146\n",
      "tensor(0.2934, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.29343441128730774 dis_loss: 0.5912246108055115\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5914, grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.29273390769958496 dis_loss: 0.5913997888565063\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5905, grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.2920311391353607 dis_loss: 0.5905135869979858\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5880, grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.29331469535827637 dis_loss: 0.5880458354949951\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5872, grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.29368144273757935 dis_loss: 0.5872421264648438\n",
      "tensor(0.2899, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.2898724675178528 dis_loss: 0.5907699465751648\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5928, grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.29188597202301025 dis_loss: 0.5928059816360474\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5913, grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.29276251792907715 dis_loss: 0.5912661552429199\n",
      "tensor(0.2931, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.2931080162525177 dis_loss: 0.5891205668449402\n",
      "tensor(0.2910, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.2910413444042206 dis_loss: 0.5898133516311646\n",
      "tensor(0.2921, grad_fn=<SumBackward0>) tensor(0.5920, grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.29207685589790344 dis_loss: 0.5919766426086426\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5885, grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.29379862546920776 dis_loss: 0.5884546041488647\n",
      "tensor(0.2930, grad_fn=<SumBackward0>) tensor(0.5934, grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.2929992079734802 dis_loss: 0.5933811068534851\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.29247790575027466 dis_loss: 0.5896410942077637\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5876, grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.29263556003570557 dis_loss: 0.5875672101974487\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.2938494384288788 dis_loss: 0.5904196500778198\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5887, grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.29383760690689087 dis_loss: 0.5887191295623779\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5888, grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.29416215419769287 dis_loss: 0.5888105034828186\n",
      "tensor(0.2940, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.2940234839916229 dis_loss: 0.59101802110672\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5933, grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.29385218024253845 dis_loss: 0.5932810306549072\n",
      "tensor(0.2949, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.29486578702926636 dis_loss: 0.5882571935653687\n",
      "tensor(0.2941, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.29412001371383667 dis_loss: 0.5882669687271118\n",
      "tensor(0.2918, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.29177552461624146 dis_loss: 0.5901490449905396\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.29197999835014343 dis_loss: 0.590612530708313\n",
      "tensor(0.2909, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.2909374535083771 dis_loss: 0.5889623165130615\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5875, grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.29329073429107666 dis_loss: 0.587465763092041\n",
      "tensor(0.2936, grad_fn=<SumBackward0>) tensor(0.5911, grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.2936292290687561 dis_loss: 0.5910588502883911\n",
      "tensor(0.2951, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.2950991988182068 dis_loss: 0.5896368026733398\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5924, grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.2927576005458832 dis_loss: 0.5923659801483154\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.29193124175071716 dis_loss: 0.5911640524864197\n",
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.2934950292110443 dis_loss: 0.591027557849884\n",
      "tensor(0.2941, grad_fn=<SumBackward0>) tensor(0.5921, grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.29410967230796814 dis_loss: 0.5920970439910889\n",
      "tensor(0.2899, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.28987762331962585 dis_loss: 0.5895348787307739\n",
      "tensor(0.2934, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.2933962345123291 dis_loss: 0.5909765958786011\n",
      "tensor(0.2914, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.29142457246780396 dis_loss: 0.5901083946228027\n",
      "tensor(0.2916, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.2916470766067505 dis_loss: 0.5901050567626953\n",
      "tensor(0.2898, grad_fn=<SumBackward0>) tensor(0.5913, grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.28976449370384216 dis_loss: 0.5913274884223938\n",
      "tensor(0.2922, grad_fn=<SumBackward0>) tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.29218974709510803 dis_loss: 0.5899292230606079\n",
      "tensor(0.2921, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.2920726239681244 dis_loss: 0.5883384943008423\n",
      "tensor(0.2915, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.29147467017173767 dis_loss: 0.589593231678009\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5867, grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.2937178611755371 dis_loss: 0.5866729021072388\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.2927129864692688 dis_loss: 0.5909565687179565\n",
      "tensor(0.2912, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.29121699929237366 dis_loss: 0.5883097648620605\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5923, grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.29270392656326294 dis_loss: 0.5922726392745972\n",
      "tensor(0.2941, grad_fn=<SumBackward0>) tensor(0.5936, grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.294055700302124 dis_loss: 0.5935945510864258\n",
      "tensor(0.2945, grad_fn=<SumBackward0>) tensor(0.5887, grad_fn=<AddBackward0>)\n",
      "50th iteration gen_loss: 0.29445773363113403 dis_loss: 0.5887067317962646\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "50th iteration gen_loss: 0.2933371961116791 dis_loss: 0.5894672274589539\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5932, grad_fn=<AddBackward0>)\n",
      "51th iteration gen_loss: 0.2919735312461853 dis_loss: 0.5931605100631714\n",
      "tensor(0.2930, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "51th iteration gen_loss: 0.29299941658973694 dis_loss: 0.5895006656646729\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "52th iteration gen_loss: 0.2929457128047943 dis_loss: 0.591192364692688\n",
      "tensor(0.2954, grad_fn=<SumBackward0>) tensor(0.5880, grad_fn=<AddBackward0>)\n",
      "52th iteration gen_loss: 0.2954186797142029 dis_loss: 0.5880192518234253\n",
      "tensor(0.2924, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "53th iteration gen_loss: 0.2923586070537567 dis_loss: 0.5895546674728394\n",
      "tensor(0.2915, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "53th iteration gen_loss: 0.2915402948856354 dis_loss: 0.5909925699234009\n",
      "tensor(0.2918, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "54th iteration gen_loss: 0.2917793393135071 dis_loss: 0.589139461517334\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "54th iteration gen_loss: 0.29391393065452576 dis_loss: 0.5910425186157227\n",
      "tensor(0.2936, grad_fn=<SumBackward0>) tensor(0.5887, grad_fn=<AddBackward0>)\n",
      "55th iteration gen_loss: 0.29364365339279175 dis_loss: 0.5887105464935303\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "55th iteration gen_loss: 0.29375898838043213 dis_loss: 0.5897871255874634\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5884, grad_fn=<AddBackward0>)\n",
      "56th iteration gen_loss: 0.29269564151763916 dis_loss: 0.5884217619895935\n",
      "tensor(0.2924, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "56th iteration gen_loss: 0.29243749380111694 dis_loss: 0.5898457765579224\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5871, grad_fn=<AddBackward0>)\n",
      "57th iteration gen_loss: 0.29185110330581665 dis_loss: 0.5871320962905884\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5883, grad_fn=<AddBackward0>)\n",
      "57th iteration gen_loss: 0.2925390601158142 dis_loss: 0.588287889957428\n",
      "tensor(0.2906, grad_fn=<SumBackward0>) tensor(0.5916, grad_fn=<AddBackward0>)\n",
      "58th iteration gen_loss: 0.29062360525131226 dis_loss: 0.5915780663490295\n",
      "tensor(0.2898, grad_fn=<SumBackward0>) tensor(0.5922, grad_fn=<AddBackward0>)\n",
      "58th iteration gen_loss: 0.28982818126678467 dis_loss: 0.5921732783317566\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5903, grad_fn=<AddBackward0>)\n",
      "59th iteration gen_loss: 0.2942383885383606 dis_loss: 0.5902515649795532\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5917, grad_fn=<AddBackward0>)\n",
      "59th iteration gen_loss: 0.2937283515930176 dis_loss: 0.5917073488235474\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5924, grad_fn=<AddBackward0>)\n",
      "60th iteration gen_loss: 0.291982501745224 dis_loss: 0.5923547744750977\n",
      "tensor(0.2910, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "60th iteration gen_loss: 0.2909950315952301 dis_loss: 0.5890262126922607\n",
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "61th iteration gen_loss: 0.2935153841972351 dis_loss: 0.5895614624023438\n",
      "tensor(0.2931, grad_fn=<SumBackward0>) tensor(0.5889, grad_fn=<AddBackward0>)\n",
      "61th iteration gen_loss: 0.29314321279525757 dis_loss: 0.5889031291007996\n",
      "tensor(0.2940, grad_fn=<SumBackward0>) tensor(0.5917, grad_fn=<AddBackward0>)\n",
      "62th iteration gen_loss: 0.2940448224544525 dis_loss: 0.5917103290557861\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "62th iteration gen_loss: 0.2924724817276001 dis_loss: 0.590370774269104\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "63th iteration gen_loss: 0.2932811975479126 dis_loss: 0.5890370607376099\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5913, grad_fn=<AddBackward0>)\n",
      "63th iteration gen_loss: 0.29195520281791687 dis_loss: 0.5912989974021912\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5873, grad_fn=<AddBackward0>)\n",
      "64th iteration gen_loss: 0.29275819659233093 dis_loss: 0.587283730506897\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5870, grad_fn=<AddBackward0>)\n",
      "64th iteration gen_loss: 0.29373735189437866 dis_loss: 0.5869680643081665\n",
      "tensor(0.2932, grad_fn=<SumBackward0>) tensor(0.5881, grad_fn=<AddBackward0>)\n",
      "65th iteration gen_loss: 0.293211430311203 dis_loss: 0.5881344676017761\n",
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.5882, grad_fn=<AddBackward0>)\n",
      "65th iteration gen_loss: 0.2934710383415222 dis_loss: 0.5881734490394592\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5909, grad_fn=<AddBackward0>)\n",
      "66th iteration gen_loss: 0.2937544584274292 dis_loss: 0.5909126996994019\n",
      "tensor(0.2936, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "66th iteration gen_loss: 0.2935987114906311 dis_loss: 0.5895708799362183\n",
      "tensor(0.2955, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "67th iteration gen_loss: 0.2955065965652466 dis_loss: 0.5894567370414734\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5907, grad_fn=<AddBackward0>)\n",
      "67th iteration gen_loss: 0.2936617136001587 dis_loss: 0.5906938910484314\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5899, grad_fn=<AddBackward0>)\n",
      "68th iteration gen_loss: 0.2928584814071655 dis_loss: 0.5899406671524048\n",
      "tensor(0.2907, grad_fn=<SumBackward0>) tensor(0.5907, grad_fn=<AddBackward0>)\n",
      "68th iteration gen_loss: 0.2907467484474182 dis_loss: 0.5907027721405029\n",
      "tensor(0.2935, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "69th iteration gen_loss: 0.2934751808643341 dis_loss: 0.5903910398483276\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "69th iteration gen_loss: 0.29419729113578796 dis_loss: 0.5898435115814209\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "70th iteration gen_loss: 0.291932612657547 dis_loss: 0.5889772176742554\n",
      "tensor(0.2922, grad_fn=<SumBackward0>) tensor(0.5917, grad_fn=<AddBackward0>)\n",
      "70th iteration gen_loss: 0.2921748757362366 dis_loss: 0.5917423963546753\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5878, grad_fn=<AddBackward0>)\n",
      "71th iteration gen_loss: 0.29276221990585327 dis_loss: 0.5878021717071533\n",
      "tensor(0.2914, grad_fn=<SumBackward0>) tensor(0.5874, grad_fn=<AddBackward0>)\n",
      "71th iteration gen_loss: 0.29140323400497437 dis_loss: 0.5874441862106323\n",
      "tensor(0.2912, grad_fn=<SumBackward0>) tensor(0.5867, grad_fn=<AddBackward0>)\n",
      "72th iteration gen_loss: 0.29124915599823 dis_loss: 0.586701512336731\n",
      "tensor(0.2907, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "72th iteration gen_loss: 0.2906993329524994 dis_loss: 0.5905565023422241\n",
      "tensor(0.2903, grad_fn=<SumBackward0>) tensor(0.5894, grad_fn=<AddBackward0>)\n",
      "73th iteration gen_loss: 0.2902691960334778 dis_loss: 0.589447557926178\n",
      "tensor(0.2906, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "73th iteration gen_loss: 0.2906026244163513 dis_loss: 0.5899696350097656\n",
      "tensor(0.2936, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "74th iteration gen_loss: 0.29360973834991455 dis_loss: 0.5905604958534241\n",
      "tensor(0.2914, grad_fn=<SumBackward0>) tensor(0.5905, grad_fn=<AddBackward0>)\n",
      "74th iteration gen_loss: 0.29137885570526123 dis_loss: 0.5905332565307617\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "75th iteration gen_loss: 0.2928679287433624 dis_loss: 0.5907581448554993\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "75th iteration gen_loss: 0.29262205958366394 dis_loss: 0.5896463394165039\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "76th iteration gen_loss: 0.29375404119491577 dis_loss: 0.5912224054336548\n",
      "tensor(0.2931, grad_fn=<SumBackward0>) tensor(0.5903, grad_fn=<AddBackward0>)\n",
      "76th iteration gen_loss: 0.2930655777454376 dis_loss: 0.5903122425079346\n",
      "tensor(0.2950, grad_fn=<SumBackward0>) tensor(0.5926, grad_fn=<AddBackward0>)\n",
      "77th iteration gen_loss: 0.29501911997795105 dis_loss: 0.5925969481468201\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "77th iteration gen_loss: 0.29252418875694275 dis_loss: 0.5895246863365173\n",
      "tensor(0.2927, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "78th iteration gen_loss: 0.2926735281944275 dis_loss: 0.5904197692871094\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "78th iteration gen_loss: 0.2938689887523651 dis_loss: 0.5899988412857056\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5907, grad_fn=<AddBackward0>)\n",
      "79th iteration gen_loss: 0.2929426431655884 dis_loss: 0.5906734466552734\n",
      "tensor(0.2898, grad_fn=<SumBackward0>) tensor(0.5913, grad_fn=<AddBackward0>)\n",
      "79th iteration gen_loss: 0.2898132801055908 dis_loss: 0.5913351774215698\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5903, grad_fn=<AddBackward0>)\n",
      "80th iteration gen_loss: 0.2918769419193268 dis_loss: 0.5903371572494507\n",
      "tensor(0.2893, grad_fn=<SumBackward0>) tensor(0.5912, grad_fn=<AddBackward0>)\n",
      "80th iteration gen_loss: 0.2892508804798126 dis_loss: 0.5912230610847473\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5861, grad_fn=<AddBackward0>)\n",
      "81th iteration gen_loss: 0.29293525218963623 dis_loss: 0.5860946178436279\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "81th iteration gen_loss: 0.2922874391078949 dis_loss: 0.5899564027786255\n",
      "tensor(0.2937, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "82th iteration gen_loss: 0.29366734623908997 dis_loss: 0.5901122689247131\n",
      "tensor(0.2920, grad_fn=<SumBackward0>) tensor(0.5909, grad_fn=<AddBackward0>)\n",
      "82th iteration gen_loss: 0.29204919934272766 dis_loss: 0.5909406542778015\n",
      "tensor(0.2955, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "83th iteration gen_loss: 0.29545658826828003 dis_loss: 0.589838981628418\n",
      "tensor(0.2886, grad_fn=<SumBackward0>) tensor(0.5902, grad_fn=<AddBackward0>)\n",
      "83th iteration gen_loss: 0.2886376678943634 dis_loss: 0.590154767036438\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5869, grad_fn=<AddBackward0>)\n",
      "84th iteration gen_loss: 0.2923092544078827 dis_loss: 0.5868540406227112\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "84th iteration gen_loss: 0.2927708327770233 dis_loss: 0.5890753269195557\n",
      "tensor(0.2941, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "85th iteration gen_loss: 0.2940662205219269 dis_loss: 0.5907717943191528\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5891, grad_fn=<AddBackward0>)\n",
      "85th iteration gen_loss: 0.293943852186203 dis_loss: 0.5891420245170593\n",
      "tensor(0.2912, grad_fn=<SumBackward0>) tensor(0.5896, grad_fn=<AddBackward0>)\n",
      "86th iteration gen_loss: 0.2911701202392578 dis_loss: 0.5895981788635254\n",
      "tensor(0.2917, grad_fn=<SumBackward0>) tensor(0.5918, grad_fn=<AddBackward0>)\n",
      "86th iteration gen_loss: 0.29165175557136536 dis_loss: 0.5917892456054688\n",
      "tensor(0.2908, grad_fn=<SumBackward0>) tensor(0.5939, grad_fn=<AddBackward0>)\n",
      "87th iteration gen_loss: 0.2908279299736023 dis_loss: 0.593948483467102\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5911, grad_fn=<AddBackward0>)\n",
      "87th iteration gen_loss: 0.29190436005592346 dis_loss: 0.5911015272140503\n",
      "tensor(0.2948, grad_fn=<SumBackward0>) tensor(0.5903, grad_fn=<AddBackward0>)\n",
      "88th iteration gen_loss: 0.2947905957698822 dis_loss: 0.5902506113052368\n",
      "tensor(0.2922, grad_fn=<SumBackward0>) tensor(0.5926, grad_fn=<AddBackward0>)\n",
      "88th iteration gen_loss: 0.2922106385231018 dis_loss: 0.5925874710083008\n",
      "tensor(0.2911, grad_fn=<SumBackward0>) tensor(0.5924, grad_fn=<AddBackward0>)\n",
      "89th iteration gen_loss: 0.29112738370895386 dis_loss: 0.592437744140625\n",
      "tensor(0.2907, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "89th iteration gen_loss: 0.290732741355896 dis_loss: 0.5903970003128052\n",
      "tensor(0.2921, grad_fn=<SumBackward0>) tensor(0.5894, grad_fn=<AddBackward0>)\n",
      "90th iteration gen_loss: 0.29210150241851807 dis_loss: 0.5893591642379761\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5926, grad_fn=<AddBackward0>)\n",
      "90th iteration gen_loss: 0.29261285066604614 dis_loss: 0.5926036238670349\n",
      "tensor(0.2933, grad_fn=<SumBackward0>) tensor(0.5890, grad_fn=<AddBackward0>)\n",
      "91th iteration gen_loss: 0.29331570863723755 dis_loss: 0.5890140533447266\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "91th iteration gen_loss: 0.29227274656295776 dis_loss: 0.5895068645477295\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "92th iteration gen_loss: 0.29387930035591125 dis_loss: 0.5894864797592163\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5898, grad_fn=<AddBackward0>)\n",
      "92th iteration gen_loss: 0.2923189103603363 dis_loss: 0.5898160934448242\n",
      "tensor(0.2930, grad_fn=<SumBackward0>) tensor(0.5885, grad_fn=<AddBackward0>)\n",
      "93th iteration gen_loss: 0.29303935170173645 dis_loss: 0.5884897708892822\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "93th iteration gen_loss: 0.29286989569664 dis_loss: 0.5908271074295044\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5893, grad_fn=<AddBackward0>)\n",
      "94th iteration gen_loss: 0.2926286458969116 dis_loss: 0.5893476605415344\n",
      "tensor(0.2928, grad_fn=<SumBackward0>) tensor(0.5893, grad_fn=<AddBackward0>)\n",
      "94th iteration gen_loss: 0.29278090596199036 dis_loss: 0.5893127918243408\n",
      "tensor(0.2931, grad_fn=<SumBackward0>) tensor(0.5911, grad_fn=<AddBackward0>)\n",
      "95th iteration gen_loss: 0.2930891811847687 dis_loss: 0.591096043586731\n",
      "tensor(0.2925, grad_fn=<SumBackward0>) tensor(0.5904, grad_fn=<AddBackward0>)\n",
      "95th iteration gen_loss: 0.29247909784317017 dis_loss: 0.5904097557067871\n",
      "tensor(0.2953, grad_fn=<SumBackward0>) tensor(0.5915, grad_fn=<AddBackward0>)\n",
      "96th iteration gen_loss: 0.29534628987312317 dis_loss: 0.5914826989173889\n",
      "tensor(0.2921, grad_fn=<SumBackward0>) tensor(0.5897, grad_fn=<AddBackward0>)\n",
      "96th iteration gen_loss: 0.29211023449897766 dis_loss: 0.5896755456924438\n",
      "tensor(0.2918, grad_fn=<SumBackward0>) tensor(0.5908, grad_fn=<AddBackward0>)\n",
      "97th iteration gen_loss: 0.2918250262737274 dis_loss: 0.5907881855964661\n",
      "tensor(0.2938, grad_fn=<SumBackward0>) tensor(0.5902, grad_fn=<AddBackward0>)\n",
      "97th iteration gen_loss: 0.2938241958618164 dis_loss: 0.5901908874511719\n",
      "tensor(0.2952, grad_fn=<SumBackward0>) tensor(0.5901, grad_fn=<AddBackward0>)\n",
      "98th iteration gen_loss: 0.2952346205711365 dis_loss: 0.5901105403900146\n",
      "tensor(0.2939, grad_fn=<SumBackward0>) tensor(0.5906, grad_fn=<AddBackward0>)\n",
      "98th iteration gen_loss: 0.2939009368419647 dis_loss: 0.5905740857124329\n",
      "tensor(0.2915, grad_fn=<SumBackward0>) tensor(0.5885, grad_fn=<AddBackward0>)\n",
      "99th iteration gen_loss: 0.29150834679603577 dis_loss: 0.5884808301925659\n",
      "tensor(0.2934, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "99th iteration gen_loss: 0.2933691442012787 dis_loss: 0.5910239219665527\n",
      "tensor(0.2916, grad_fn=<SumBackward0>) tensor(0.5895, grad_fn=<AddBackward0>)\n",
      "100th iteration gen_loss: 0.29161888360977173 dis_loss: 0.589479923248291\n",
      "tensor(0.2923, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "100th iteration gen_loss: 0.2923000156879425 dis_loss: 0.589963436126709\n",
      "tensor(0.2919, grad_fn=<SumBackward0>) tensor(0.5900, grad_fn=<AddBackward0>)\n",
      "101th iteration gen_loss: 0.291917085647583 dis_loss: 0.589956521987915\n",
      "tensor(0.2926, grad_fn=<SumBackward0>) tensor(0.5910, grad_fn=<AddBackward0>)\n",
      "101th iteration gen_loss: 0.292553186416626 dis_loss: 0.5909596681594849\n",
      "tensor(0.2942, grad_fn=<SumBackward0>) tensor(0.5905, grad_fn=<AddBackward0>)\n",
      "102th iteration gen_loss: 0.29417604207992554 dis_loss: 0.590514063835144\n",
      "tensor(0.2930, grad_fn=<SumBackward0>) tensor(0.5915, grad_fn=<AddBackward0>)\n",
      "102th iteration gen_loss: 0.2929953634738922 dis_loss: 0.5915360450744629\n",
      "tensor(0.2929, grad_fn=<SumBackward0>) tensor(0.5916, grad_fn=<AddBackward0>)\n",
      "103th iteration gen_loss: 0.29293936491012573 dis_loss: 0.5916165113449097\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j,(image,label) in enumerate(train_loader):\n",
    "        image = Variable(image)\n",
    "        \n",
    "        # discriminator\n",
    "        \n",
    "        dis_optim.zero_grad()\n",
    "        \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1))\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        dis_real = discriminator.forward(image)\n",
    "        dis_loss = torch.sum(loss_func(dis_fake,zeros_label)) + torch.sum(loss_func(dis_real,ones_label))\n",
    "        dis_loss.backward(retain_graph=True)\n",
    "        dis_optim.step()\n",
    "        \n",
    "        # generator\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size,z_size),mean=0,std=0.1))\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        gen_loss = torch.sum(loss_func(dis_fake,ones_label)) # fake classified as real\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "    \n",
    "       \n",
    "    \n",
    "        # model save\n",
    "        if j % 100 == 0:\n",
    "            print(gen_loss,dis_loss)\n",
    "            torch.save([generator,discriminator],'./model/vanilla_gan.pkl')\n",
    "\n",
    "            print(\"{}th iteration gen_loss: {} dis_loss: {}\".format(i,gen_loss.data,dis_loss.data))\n",
    "            v_utils.save_image(gen_fake.data[0:25],\"./result/gen_{}_{}.png\".format(i,j), nrow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
